{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shit\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train) 1\n",
      "len(TEXT.vocab) 10001\n"
     ]
    }
   ],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "print('len(train)', len(train))\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate perplexity\n",
    "def perplexity(pred, true):\n",
    "    nll = nn.NLLLoss()\n",
    "    loss = nll(pred,true)\n",
    "    perplexity = np.exp(loss.data/pred.size()[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM that predicts based on last hidden state\n",
    "class LCA_LSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, batch_size, layers = 2):\n",
    "        \n",
    "        super(LCA_LSTM,self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.layers = layers\n",
    "        \n",
    "        #define layers of our model\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        \n",
    "        #self.embedding.weight.data = TEXT.vocab.vectors\n",
    "        #self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,layers,dropout=0.5)\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.final = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.d1 = nn.Dropout(p=0.5)\n",
    "        self.d2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fresh = True\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (Variable(torch.zeros(self.layers,self.batch_size,self.hidden_dim).cuda()),Variable(torch.zeros(self.layers,self.batch_size,self.hidden_dim).cuda()))\n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        \n",
    "        #get embedded vectors\n",
    "        vectors = self.embedding(sentence)\n",
    "        \n",
    "        #vectors = self.d1(vectors)\n",
    "        \n",
    "        #clean hidden layer\n",
    "        if self.fresh:\n",
    "            self.hidden = self.init_hidden()\n",
    "        \n",
    "        #pass through the entire sentence\n",
    "        seq, self.hidden = self.lstm(vectors, tuple(state.detach() for state in self.hidden))\n",
    "        \n",
    "        #project from hidden state to word space\n",
    "        scores = [self.final(self.d1(h_t)) for h_t in seq]\n",
    "        \n",
    "        scores = torch.cat(scores)\n",
    "        \n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to train lstm model\n",
    "def train_lstm(best_model):\n",
    "    \n",
    "    #get certain constants\n",
    "    vocab_size = len(TEXT.vocab)\n",
    "    train_len = len(train_iter)\n",
    "    val_len = len(val_iter)\n",
    "    epochs = 20\n",
    "    pad_idx = TEXT.vocab.stoi[\"<pad>\"]\n",
    "    batch_size = 10\n",
    "    \n",
    "    #create model\n",
    "    model = LCA_LSTM(500, 200, len(TEXT.vocab), batch_size, 3).cuda()\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss(size_average=True)\n",
    "    loss_function2 = nn.NLLLoss(size_average=False)\n",
    "    #optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.05)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr = 0.001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.fresh = True\n",
    "        \n",
    "        num_train_batches = 0\n",
    "        total_train_loss = 0.0\n",
    "            \n",
    "        for batch in train_iter:\n",
    "                      \n",
    "            if num_train_batches is not 0:\n",
    "                model.fresh = False\n",
    "                \n",
    "            num_train_batches += 1  \n",
    "            \n",
    "            with torch.cuda.device(0):\n",
    "                model.zero_grad()\n",
    "            \n",
    "                log_probs = model(batch.text.cuda())\n",
    "            \n",
    "                loss = loss_function(log_probs, batch.target.view(-1).cuda())\n",
    "            \n",
    "                #retain graph if its not the last batch\n",
    "                loss.backward(retain_graph=(num_train_batches is not train_len))\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.data\n",
    "            \n",
    "            if num_train_batches >= train_len:\n",
    "                break\n",
    "        \n",
    "        #set fresh to wipe hidden state for validation run\n",
    "        model.fresh = True\n",
    "        \n",
    "        #accumulate loss then divide by number of batches\n",
    "        num_val_batches = 0.0\n",
    "        num_val_words = 0.0\n",
    "        total_val_loss = 0.0\n",
    "        total_val_nll = 0.0\n",
    "        \n",
    "        #calculate loss and perplexity on language model\n",
    "        for batch in val_iter:\n",
    "            \n",
    "            non_pad = batch.target.ne(pad_idx).cuda().view(-1)\n",
    "            \n",
    "            log_probs = model(batch.text.cuda())\n",
    "            \n",
    "            loss = loss_function(log_probs, batch.target.cuda().view(-1))\n",
    "            \n",
    "            total_val_loss += loss.data[0]\n",
    "            \n",
    "            non_pad_probs = log_probs[non_pad.nonzero(),:]\n",
    "            non_pad_true = batch.target.cuda().view(-1)[non_pad]\n",
    "                \n",
    "            loss2 = loss_function(non_pad_probs.squeeze(), non_pad_true)\n",
    "\n",
    "            total_val_nll += loss2.data[0]\n",
    "            \n",
    "            num_val_words += batch.target.ne(pad_idx).sum().data[0]\n",
    "            num_val_batches += 1\n",
    "                \n",
    "            if num_val_batches >= val_len:\n",
    "                break\n",
    "            \n",
    "        #calculate perplexity\n",
    "        perp = np.exp((total_val_nll)/num_val_batches)\n",
    "        \n",
    "        #report statistics\n",
    "        print(\"Epoch %d: Avg Val Loss (X-E): %f, Val Perplexity: %f, Avg Train Loss: %f\" % (epoch,total_val_loss/num_val_batches,perp,total_train_loss/num_train_batches))\n",
    "        \n",
    "        if perp < best_model['perp']:\n",
    "            best_model['model'] = model\n",
    "            best_model['perp'] = perp\n",
    "\n",
    "    return model\n",
    "\n",
    "my_dict = {'model':None, 'perp':np.inf}\n",
    "my_model = train_lstm(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LCA_LSTM(\n",
       "  (embedding): Embedding(10001, 300)\n",
       "  (lstm): LSTM(300, 50, num_layers=2)\n",
       "  (final): Linear(in_features=50, out_features=10001)\n",
       "  (d1): Dropout(p=0.5)\n",
       "  (d2): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2lstm=my_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = next(iter(train_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       "[torch.LongTensor of size 1x1]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ByteTensor([1,0]).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.085536923187668"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
