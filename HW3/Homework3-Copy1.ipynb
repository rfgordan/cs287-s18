{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import numpy as np\n",
    "import torch.nn.init as weight_init\n",
    "import time\n",
    "import os\n",
    "import heapq as hq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7fdf7c0754a8>, 'src': <torchtext.data.field.Field object at 0x7fdf7c0754e0>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    28     12     28     23  10580    475   1714      9     12     12     87\n",
      "  1312      6     33      0     24      9   1090   8924     43    280    258\n",
      "     0    637    106    199    141      3     11     19      4     11     19\n",
      "    36     25      5      0      4   4854   1023     37      7    255    285\n",
      "     5     64   1527    412     19   6804      8    101  12555   3834     41\n",
      "     0   1941      7     62    757    446    898     71     38    158    379\n",
      "    53   1012   1061    975  11845     19    510      0    162   6443      0\n",
      "     2      2      2      2      2      2      2      2     16      2     16\n",
      "\n",
      "Columns 11 to 21 \n",
      "    12    252     20    218      9    373     12     20    287     77   1417\n",
      "     6     60   2358   5039      8    244    899     93   1202    188    378\n",
      "   103     10    290     13    205     13    184      3      4     19     21\n",
      "     9     51      3    211   8398    119      3     43     29    128     10\n",
      "    66    953     10     14     33    251    210    188    113     37    296\n",
      "    85     65    513      6   2693      3      9     21   2380    263    566\n",
      "   119   4029      0    582    566    224   5080    118      0      6    101\n",
      "     2      2      2      2      2      2      2     16      2     16      2\n",
      "\n",
      "Columns 22 to 31 \n",
      "    20     78    405     23   1025      0     12     26     73     23\n",
      "   197     37      5     27      4      0     44      4      3     35\n",
      "    10     21   2892    165      6     24    115      7   3729   2211\n",
      "   135      3      3      0   3558     12    109   1505     14     22\n",
      "    29     27     79     36    634     41     93    389      7    538\n",
      "    46   5892      5     19    199    116      0     30      0      0\n",
      "   544   4553   2814   3554   6945     10   2231   2439   1962    264\n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "[torch.LongTensor of size 8x32]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "    34     14     34     42      0    576     41     57     14    825    154\n",
      "  1218    497     28      0     39      5    773   5446     32     10    111\n",
      "  3318      8      6      9     20   3387      5    254     12     62      8\n",
      "  1389    241    984     50     11   3313     10     51      6    313    324\n",
      "  4348    475      9   5454      8     69    102      8   1098     35    102\n",
      "    29      4   1310   3900    339     38     71    896     38     20     61\n",
      "     6      3     91     59   6371    433     18     12     68     21    308\n",
      "  1389      1    590   2395      4      4     10   2351     21      3     61\n",
      "   883      1     94      4      3      3    225      4      3      1    674\n",
      "     4      1      4      3      1      1    100      3      1      1     21\n",
      "     3      1      3      1      1      1      4      1      1      1      3\n",
      "     1      1      1      1      1      1      3      1      1      1      1\n",
      "     1      1      1      1      1      1      1      1      1      1      1\n",
      "\n",
      "Columns 11 to 21 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "    14     97     10     76     52    192     14    764     48    154     14\n",
      "    67     12   2767     19     18      5     15      5    897    125    234\n",
      "    12      8    207     43     82    169    138     32      5     16      5\n",
      "    32    184     16   6501  10003     19     72     11     16    140      6\n",
      "    15     35     23    105      5     62     68    149     88     21    275\n",
      "   156   6376      0    136     15    162    161    159    114      3    788\n",
      "   126      4      4     69     28      4    141     21   1873      1    104\n",
      "     7      3      3      4      8      3     15      3      4      1    637\n",
      "    62      1      1      3   2515      1    274      1      3      1      4\n",
      "    78      1      1      1     17      1      4      1      1      1      3\n",
      "     4      1      1      1    653      1      3      1      1      1      1\n",
      "     3      1      1      1      4      1      1      1      1      1      1\n",
      "     1      1      1      1      3      1      1      1      1      1      1\n",
      "\n",
      "Columns 22 to 31 \n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "    10    367    469     48   7723      0     14     27     24    652\n",
      "    74     51     29     11     11      0    137     12      5     23\n",
      "    16     20      6      5      6     39    450      6   2259    202\n",
      "    88     25   4792    205   6870     14      5    531      7   5102\n",
      "     7     61      5      5      9     13    137     33      6    203\n",
      "   210   9767     93     29     46     12     77   3474   5822      5\n",
      "    38      4     29      8     50     85     51      4   7636    800\n",
      "    94      3      6   1975   2538     16      5      3      4      4\n",
      "     4      1   2585      5      4    307   2068      1      3      3\n",
      "     3      1      4   7921      3      4   2825      1      1      1\n",
      "     1      1      3   1819      1      3      4      1      1      1\n",
      "     1      1      1      4      1      1      3      1      1      1\n",
      "     1      1      1      3      1      1      1      1      1      1\n",
      "[torch.LongTensor of size 14x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\r\n",
      "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\r\n",
      "Sie war eine 26-jährige Frau namens Alex .\r\n",
      "Und als ich das hörte , war ich erleichtert .\r\n",
      "Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten .\r\n",
      "Und ich bekam eine Frau in den 20ern , die über Jungs reden wollte .\r\n",
      "Das kriege ich hin , dachte ich mir .\r\n",
      "Aber ich habe es nicht hingekriegt .\r\n",
      "Arbeit kam später , Heiraten kam später , Kinder kamen später , selbst der Tod kam später .\r\n",
      "Leute in den 20ern wie Alex und ich hatten nichts als Zeit .\r\n"
     ]
    }
   ],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder, use_true = True):\n",
    "        super(Seq2SeqAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.last_hidden_enc = (self.encoder.num_layers*(self.encoder.bidirectional*2)) - 1\n",
    "        self.use_true = use_true # want to feed true last words in when training\n",
    "        \n",
    "    def forward(self, input_seqs, target_seqs):\n",
    "        batch_size = input_seqs.size(1)\n",
    "        target_length = target_seqs.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = Variable(torch.zeros(target_length, batch_size, vocab_size)).cuda()\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seqs, None)\n",
    "        \n",
    "        output = Variable(target_seqs.data[0, :])\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            \n",
    "            \n",
    "            #context = Variable(torch.zeros(batch_size,self.encoder.hidden_size)).cuda()\n",
    "            \n",
    "            #print(context.size())\n",
    "            \n",
    "            encoder_output_attn = encoder_output.transpose(0,1)\n",
    "            hidden_attn = hidden[0][self.last_hidden_enc,:,:]\n",
    "            \n",
    "            hidden_attn=hidden_attn.unsqueeze(2)\n",
    "            #print(hidden_attn.size(), encoder_output_attn.size())\n",
    "            att_probs = torch.bmm(encoder_output_attn, hidden_attn)\n",
    "            context = torch.bmm(att_probs.transpose(1,2), encoder_output_attn)\n",
    "            \n",
    "#             #loop over examples in batch\n",
    "#             for i in range(batch_size):\n",
    "                \n",
    "#                 #print(encoder_output[:,i,:].size())\n",
    "#                 #print(hidden[0][1,i,:].size())\n",
    "                \n",
    "#                 #calculate attention probabilities\n",
    "                \n",
    "#                 att_probs = torch.matmul(encoder_output[:,i,:],hidden[0][self.last_hidden_enc,i,:])\n",
    "                \n",
    "#                 #print(att_probs.size())\n",
    "                \n",
    "#                 #now get \"expected\" for each element in batch\n",
    "#                 context[i,:] = torch.matmul(att_probs,encoder_output[:,i,:])\n",
    "            \n",
    "            \n",
    "            output, hidden = self.decoder(output, hidden, context)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            #use true values only if training/validating model\n",
    "            if self.use_true:\n",
    "                output = Variable(target_seqs.data[t]).cuda()\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def batch_train(self, optimizer, train_iter, vocab_size, grad_clip=10):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        curr_time = time.time()\n",
    "        loss_f = nn.NLLLoss()\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(self.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if b % 1000 == 0 and b != 0:\n",
    "                total_loss = total_loss / 1000\n",
    "                print(\"[%d][loss:%5.2f][pp:%5.2f][time:%5.2f]\" %\n",
    "                      (b, total_loss, math.exp(total_loss), time.time() - curr_time))\n",
    "                total_loss = 0\n",
    "                curr_time = time.time()\n",
    "    \n",
    "    def beam_search(self, input_seq, beam_size, search_time, target_vocab, n = 10):\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seq, None)\n",
    "        \n",
    "        #here we track the strings still in our beam\n",
    "        track = [None]*beam_size\n",
    "        \n",
    "        # in worst case, take beam_size candidates from one former state\n",
    "        poss_next = np.min([beam_size, len(target_vocab)])\n",
    "        \n",
    "        # next states that we might want to keep around\n",
    "        possible_next = [None]*(beam_size*poss_next)\n",
    "        \n",
    "        # list of complete strings to keep around\n",
    "        final_candidates = []\n",
    "        \n",
    "        # first state we have in our beam\n",
    "        track[0] = ([target_vocab.stoi[\"<s>\"]], hidden, 0)\n",
    "        num_tracked = 1\n",
    "        \n",
    "        #start time\n",
    "        start = time.time()\n",
    "        \n",
    "        #track no. of steps we've taken\n",
    "        steps = 0\n",
    "        \n",
    "        #continue looping until we've used search time\n",
    "        while time.time() < start + search_time:\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            #track where we are in the list of possible next values\n",
    "            poss_counter = 0\n",
    "            \n",
    "            for i in range(num_tracked):\n",
    "                \n",
    "                att_probs = torch.matmul(encoder_output[:,0,:],track[i][1][0][self.last_hidden_enc,0,:].cuda())\n",
    "            \n",
    "                context = torch.matmul(att_probs,encoder_output[:,0,:].cuda())\n",
    "                \n",
    "                print(torch.LongTensor(track[i][0][-1]))\n",
    "                output, hidden = self.decoder(Variable(torch.LongTensor(track[i][0][-1])).cuda(), (x.cuda() for x in hidden), context.cuda())\n",
    "                \n",
    "                log_output = torch.log(output)\n",
    "                \n",
    "                top_next, idx = torch.topk(log_output, poss_next)\n",
    "                \n",
    "                for j in range(len(top_next)):\n",
    "                    \n",
    "                    if idx[j] != target_vocab.stoi[\"</s>\"]:\n",
    "                        \n",
    "                        possible_next[poss_counter+j] = (log_output[idx[j]] + track[i][2],top_next[j], idx[j], i, steps)\n",
    "                    \n",
    "                        poss_counter += 1\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        #we have a complete string\n",
    "                        final_candidates += [(top_next[j] + track[i][2],track[i][0]+[idx[j]])]\n",
    "                    \n",
    "                    \n",
    "            if poss_counter < beam_search:\n",
    "                \n",
    "                print(\"less possible next states than beam size. Seems unlikely this would ever happen.\")\n",
    "                \n",
    "                break\n",
    "                \n",
    "            #sort our possible next states and choose\n",
    "            hq.heapify(possible_next[:poss_counter])\n",
    "            largest = hq.nlargest(beam_size, possible_next[:poss_counter])\n",
    "            \n",
    "            num_tracked = 0\n",
    "            \n",
    "            #now loop over and put everything we want to keep in our track array\n",
    "            for k in range(poss_counter):\n",
    "                parent = track[possible_next[k][i]]\n",
    "                track[k] = (parent[0] + [possible_next[k][1]], hidden, possible_next[k][0])\n",
    "                num_tracked += 1\n",
    "                \n",
    "        #finally, sort our final candidates\n",
    "        hq.heapify(final_candidates)\n",
    "        final = hq.nlargest(n, final_candidates)\n",
    "        \n",
    "        return final\n",
    "                \n",
    "            \n",
    "        \n",
    "    def predict(self, val_iter, vocab_size):\n",
    "        self.eval()\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        total_loss = 0\n",
    "        for batch in val_iter:\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source = Variable(source.data.cuda(), volatile=True)\n",
    "                target = Variable(target.data.cuda(), volatile=True)\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            total_loss += loss.data[0]\n",
    "        return total_loss / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, context_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=False):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.context_size = context_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size+self.context_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        for param in self.lstm.parameters():\n",
    "            nn.init.uniform(param, -0.08, 0.08)\n",
    "        self.out = nn.Linear(self.hidden_size+self.context_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_seq, hidden, context):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #print(embedded.size(),context.size())\n",
    "        combined = torch.cat([embedded, context.permute(1,0,2)],2)\n",
    "        output, hidden = self.lstm(combined, hidden)\n",
    "        output = output.squeeze(0)\n",
    "        output = self.softmax(self.out(torch.cat([output,context.squeeze(1)],1)))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 13352, hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=False):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "        for param in self.lstm.parameters():\n",
    "            nn.init.uniform(param, -0.08, 0.08)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "        \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=None):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = F.tanh(output)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=False)\n",
    "decoder = AttnDecoderLSTM(hidden_size=200, context_size = 200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "seq2seq = Seq2SeqAttn(encoder, decoder).cuda()\n",
    "epoch_num = 13\n",
    "optimizer = optim.SGD(seq2seq.parameters(), lr=1)\n",
    "scheduler = MultiStepLR(optimizer, milestones=range(9, epoch_num), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000][loss: 6.17][pp:477.97][time:268.99]\n",
      "[2000][loss: 4.35][pp:77.35][time:266.40]\n",
      "[3000][loss: 3.97][pp:53.09][time:270.91]\n",
      "[Epoch:0] val_loss:3.561 | val_pp:35.18S\n",
      "[!] saving model...\n",
      "[1000][loss: 3.58][pp:35.93][time:274.41]\n",
      "[2000][loss: 3.46][pp:31.93][time:272.46]\n",
      "[3000][loss: 3.37][pp:29.12][time:272.02]\n",
      "[Epoch:1] val_loss:3.109 | val_pp:22.40S\n",
      "[!] saving model...\n",
      "[1000][loss: 3.14][pp:23.15][time:267.80]\n",
      "[2000][loss: 3.10][pp:22.28][time:271.04]\n",
      "[3000][loss: 3.04][pp:20.91][time:266.10]\n",
      "[Epoch:2] val_loss:2.897 | val_pp:18.13S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.88][pp:17.84][time:269.91]\n",
      "[2000][loss: 2.86][pp:17.47][time:269.75]\n",
      "[3000][loss: 2.82][pp:16.85][time:268.73]\n",
      "[Epoch:3] val_loss:2.754 | val_pp:15.70S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.68][pp:14.57][time:268.15]\n",
      "[2000][loss: 2.69][pp:14.67][time:269.63]\n",
      "[3000][loss: 2.68][pp:14.60][time:271.68]\n",
      "[Epoch:4] val_loss:2.662 | val_pp:14.32S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.52][pp:12.49][time:269.48]\n",
      "[2000][loss: 2.56][pp:12.95][time:270.11]\n",
      "[3000][loss: 2.56][pp:12.95][time:270.68]\n",
      "[Epoch:5] val_loss:2.601 | val_pp:13.48S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.42][pp:11.21][time:266.33]\n",
      "[2000][loss: 2.45][pp:11.55][time:273.67]\n",
      "[3000][loss: 2.46][pp:11.69][time:269.82]\n",
      "[Epoch:6] val_loss:2.649 | val_pp:14.14S\n",
      "[1000][loss: 2.31][pp:10.12][time:269.85]\n",
      "[2000][loss: 2.35][pp:10.51][time:269.59]\n",
      "[3000][loss: 2.37][pp:10.68][time:268.22]\n",
      "[Epoch:7] val_loss:2.538 | val_pp:12.66S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.23][pp: 9.34][time:274.23]\n",
      "[2000][loss: 2.29][pp: 9.84][time:268.52]\n",
      "[3000][loss: 2.29][pp: 9.90][time:271.51]\n",
      "[Epoch:8] val_loss:2.540 | val_pp:12.68S\n",
      "[1000][loss: 2.18][pp: 8.83][time:271.46]\n",
      "[2000][loss: 2.20][pp: 9.02][time:269.72]\n",
      "[3000][loss: 2.22][pp: 9.24][time:272.74]\n",
      "[Epoch:9] val_loss:2.520 | val_pp:12.43S\n",
      "[!] saving model...\n",
      "[1000][loss: 2.10][pp: 8.14][time:271.68]\n",
      "[2000][loss: 2.15][pp: 8.59][time:270.14]\n",
      "[3000][loss: 2.17][pp: 8.77][time:272.98]\n",
      "[Epoch:10] val_loss:2.525 | val_pp:12.49S\n",
      "[1000][loss: 2.04][pp: 7.71][time:268.79]\n",
      "[2000][loss: 2.10][pp: 8.14][time:269.91]\n",
      "[3000][loss: 2.12][pp: 8.32][time:271.04]\n",
      "[Epoch:11] val_loss:2.502 | val_pp:12.21S\n",
      "[!] saving model...\n",
      "[1000][loss: 1.99][pp: 7.34][time:268.59]\n",
      "[2000][loss: 2.05][pp: 7.73][time:269.22]\n",
      "[3000][loss: 2.07][pp: 7.93][time:270.79]\n",
      "[Epoch:12] val_loss:2.517 | val_pp:12.39S\n",
      "[TEST] loss: 2.71\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "#seq2seq.load_state_dict(torch.load(\"./.save/seq2seq_4.pt\"))\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 10)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        if not os.path.isdir(\".save\"):\n",
    "            os.makedirs(\".save\")\n",
    "        torch.save(seq2seq.state_dict(), './.save/seq2seq_a_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0\n",
      " 4\n",
      "[torch.LongTensor of size 2]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generic/THCTensorMathPairwise.cu:102",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-0a7264dc58cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./.save/seq2seq_a_11.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-79-2826d7d5aef6>\u001b[0m in \u001b[0;36mbeam_search\u001b[0;34m(self, input_seq, beam_size, search_time, target_vocab, n)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mlog_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-80-d2e1fd335159>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, hidden, context)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(embedded.size(),context.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/generic/THCTensorMathPairwise.cu:102"
     ]
    }
   ],
   "source": [
    "source = next(iter(test_iter)).src.cuda()\n",
    "seq2seq.load_state_dict(torch.load(\"./.save/seq2seq_a_11.pt\"))\n",
    "sentences = seq2seq.beam_search(source[:,:1], 20, 300, EN.vocab)\n",
    "for sentence in sentences:\n",
    "    print(\" \".join([EN.vocab.itos[word.data[0]] for word in sentence[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_LAUNCH_BLOCKING=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "(0 ,.,.) = \n",
       " -0.3682 -0.6084\n",
       "  0.9567 -0.7770\n",
       "  1.1498 -2.2502\n",
       "  0.4324  0.3540\n",
       " -1.6685 -0.7791\n",
       "\n",
       "(1 ,.,.) = \n",
       "  0.1256 -2.6093\n",
       "  1.4812 -0.9849\n",
       " -1.0871 -0.6905\n",
       "  0.5337 -0.0450\n",
       "  1.0682 -0.3273\n",
       "\n",
       "(2 ,.,.) = \n",
       "  1.1266  0.3800\n",
       "  0.3743  0.3221\n",
       "  0.3298  0.3691\n",
       " -0.0700 -0.1862\n",
       " -0.0530  1.0812\n",
       "\n",
       "(3 ,.,.) = \n",
       " -0.8073  0.9026\n",
       "  0.9841  1.0444\n",
       " -0.2490 -0.6544\n",
       "  0.8960  0.2368\n",
       " -0.7574  0.7852\n",
       "[torch.FloatTensor of size 4x5x2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2,5,4).permute(2,1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
