{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import numpy as np\n",
    "import torch.nn.init as weight_init\n",
    "import time\n",
    "import os\n",
    "import heapq as hq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7f324dec0ac8>, 'src': <torchtext.data.field.Field object at 0x7f32c03c4eb8>}\n",
      "119076\n",
      "{'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.'], 'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "  104    39   411    28     9    17   133    26    23    39   341   517   298\n",
      "   33     0  1678   240   111  3435  7090   192  7071     0     6    24     2\n",
      "   13    54     6    49    24     3   602    41  1693  1678    18  8660    73\n",
      "   22   434    16    18    17  1702    10   891     3    44  1073   140     3\n",
      "  641   177  6644    65  3788   704   130    53   224  4032    16   167   106\n",
      "  484   692    24    51   130   140  1451    37    21     3    23  2307   107\n",
      "   14  3260    17  1505    15    41     3     5    41   816     4    83  1447\n",
      "   71     8   204     7  1743     0   898  2832  4801     5    98   446     7\n",
      "    0  9820     2  2798  1637    16    11     0     4     0   850  2938  7239\n",
      "    2     2    17     2     2    17     2     2     2  3515     2     2     2\n",
      "\n",
      "Columns 13 to 25 \n",
      "   28    79    28    73  1844    77  1002    28     0   180    20  2675   689\n",
      "   33     9   217    85     4    89     0    33    13     0    67   151     9\n",
      "    0    47     3     0     7     9    24    21    21   393    65   623   254\n",
      "    0   100    13     3  1798     3   404    34   270   697    85    42  1385\n",
      "   34    66    54     0    25    52   216     5    92    29    53  2934     8\n",
      "    0   136  1268     0  4865   241     3     0   384     0   303    60    66\n",
      " 1807  8130   177     8   637   986     6   149  1925  4427    36    10   108\n",
      "   68  2281    85    85   149    15  4175     0    15   393    85    19   227\n",
      " 6701   444  2213  4236  6588  1378   130  3299    33   496   948     0     0\n",
      "    2     2     2     2     2    16     2     2    16     2     2     2    16\n",
      "\n",
      "Columns 26 to 31 \n",
      "   12  5486   139    40    26    28\n",
      "  226   122  7334    11  5289   302\n",
      "   54    12     3   143    62    14\n",
      "   13    84    31   205   470    72\n",
      "   66  5624    11   662    18     7\n",
      " 4354    32   164    36     3     0\n",
      "   45     6   274    51    43  1218\n",
      "    6  5392  1006   157    61     7\n",
      "   69  1184    67  2344   111   153\n",
      "    2     2     2     2     3     2\n",
      "[torch.LongTensor of size 10x32]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "   24   489   154    34   214    22    10    27   128     0   154   505   266\n",
      "   67  5119   151    43   101  3844    74   303   621    88   626    39     4\n",
      "   12  2053    28    37     5     5    20    53     5    64    11   595    24\n",
      "    8    86   120   333    22   162  2956   107     6  1662    13    15    91\n",
      "  585  1718    44    67  1895    25     0    51  2659     5    21    65     6\n",
      "    9   179    21    35   135    15    16     6   767   139    48     7  7144\n",
      "  450   445  4167     8     7    61  5401  5803  1844    11    11   163  2590\n",
      "   33   176     5  7856    82  7419     4   752   746   569    54   136     4\n",
      "    8    18    22     9   535    21     3    13   162     6   373     7     3\n",
      " 1113     0  5804  6304     4    22     1    25    16  4658     4   433     1\n",
      " 6753     4     4     4     3     3     1    53    12   161     3    38     1\n",
      "  781     3    22     3     1     1     1   169    61    18     1    92     1\n",
      "    4     1     3     1     1     1     1   700  3086    61     1     9     1\n",
      "    3     1     1     1     1     1     1     4     4  1704     1    20     1\n",
      "    1     1     1     1     1     1     1     3     3     4     1     5     1\n",
      "    1     1     1     1     1     1     1     1     1     3     1    49     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1    92     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1   256     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1    21     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     3     1\n",
      "\n",
      "Columns 13 to 25 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "   34    41    34   139    24    89  7116    34   595   175    10    14    52\n",
      "  188    15    74    11     5    26  7102    80    19    47    80    35    80\n",
      "   28    36    19    78   933    15     0   744  2846   252   120   555  6775\n",
      " 5306   211    43   269    11    26    39    16   310    45   333   117     9\n",
      "  901   471    65     5     6     7   362   158   124   311    35   243  1285\n",
      "  315    13     7    78  1465     7   853     7    26   493   325    15    18\n",
      "  996  9654   891   831     9  1346     5     6    30   557    78    43   938\n",
      "   49   230    78     5  5637    82    16     0    62   709    60    35    21\n",
      "    0     4   231    78   375   492  5445     9    85    38   358     7     3\n",
      "  582     3   176  3426     8    21   135    58     7  1456    29   696     1\n",
      "    6     1     4     4   832     3     4  3870  1031   317    78     5     1\n",
      "  106     1     3     3     4     1     3     4   193   557     4    44     1\n",
      "    4     1     1     1     3     1     1     3  1699   402     3    12     1\n",
      "    3     1     1     1     1     1     1     1    21     4     1     8     1\n",
      "    1     1     1     1     1     1     1     1     3     3     1  4480     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     4     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     3     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 26 to 31 \n",
      "    2     2     2     2     2     2\n",
      "   14    70   139    41    10    34\n",
      "    9    23   687    10    23    43\n",
      "  172    61    13    90    37   388\n",
      "    5   860    10    15    49    17\n",
      "   19     4    26    29    46    47\n",
      "  138    14    37   202   609     9\n",
      "   40    98    28  1511    17     6\n",
      "  156     6   201     4    32   153\n",
      " 3691  4220   737     3    81   860\n",
      "  134  3417     9     1    23   348\n",
      "   13   205    50     1   536    17\n",
      "    4     4   224     1     4   454\n",
      "    3     3     4     1     3     4\n",
      "    1     1     3     1     1     3\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "    1     1     1     1     1     1\n",
      "[torch.LongTensor of size 21x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Als ich in meinen 20ern war , hatte ich meine erste Psychotherapie-Patientin .\r\n",
      "Ich war Doktorandin und studierte Klinische Psychologie in Berkeley .\r\n",
      "Sie war eine 26-jährige Frau namens Alex .\r\n",
      "Und als ich das hörte , war ich erleichtert .\r\n",
      "Meine Kommilitonin bekam nämlich einen Brandstifter als ersten Patienten .\r\n",
      "Und ich bekam eine Frau in den 20ern , die über Jungs reden wollte .\r\n",
      "Das kriege ich hin , dachte ich mir .\r\n",
      "Aber ich habe es nicht hingekriegt .\r\n",
      "Arbeit kam später , Heiraten kam später , Kinder kamen später , selbst der Tod kam später .\r\n",
      "Leute in den 20ern wie Alex und ich hatten nichts als Zeit .\r\n"
     ]
    }
   ],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder, use_true = True):\n",
    "        super(Seq2SeqAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.last_hidden_enc = (self.encoder.num_layers*(self.encoder.bidirectional*2)) - 1\n",
    "        self.use_true = use_true # want to feed true last words in when training\n",
    "        \n",
    "    def forward(self, input_seqs, target_seqs):\n",
    "        batch_size = input_seqs.size(1)\n",
    "        target_length = target_seqs.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = Variable(torch.zeros(target_length, batch_size, vocab_size)).cuda()\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seqs, None)\n",
    "        \n",
    "        output = Variable(target_seqs.data[0, :])\n",
    "        \n",
    "        for t in range(1, target_length):\n",
    "            \n",
    "            \n",
    "            context = Variable(torch.zeros(batch_size,self.encoder.hidden_size)).cuda()\n",
    "            \n",
    "            #print(context.size())\n",
    "            \n",
    "            #loop over examples in batch\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                #print(encoder_output[:,i,:].size())\n",
    "                #print(hidden[0][1,i,:].size())\n",
    "                \n",
    "                #calculate attention probabilities\n",
    "                att_probs = torch.matmul(encoder_output[:,i,:],hidden[0][self.last_hidden_enc,i,:])\n",
    "                \n",
    "                #print(att_probs.size())\n",
    "                \n",
    "                #now get \"expected\" for each element in batch\n",
    "                context[i,:] = torch.matmul(att_probs,encoder_output[:,i,:])\n",
    "            \n",
    "            \n",
    "            output, hidden = self.decoder(output, hidden, context)\n",
    "            \n",
    "            outputs[t] = output\n",
    "            \n",
    "            #use true values only if training/validating model\n",
    "            if self.use_true:\n",
    "                output = Variable(target_seqs.data[t]).cuda()\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def batch_train(self, optimizer, train_iter, vocab_size, grad_clip=10):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        curr_time = time.time()\n",
    "        loss_f = nn.NLLLoss()\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(self.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if b % 1000 == 0 and b != 0:\n",
    "                total_loss = total_loss / 1000\n",
    "                print(\"[%d][loss:%5.2f][pp:%5.2f][time:%5.2f]\" %\n",
    "                      (b, total_loss, math.exp(total_loss), time.time() - curr_time))\n",
    "                total_loss = 0\n",
    "                curr_time = time.time()\n",
    "    \n",
    "    def beam_search(self, input_seq, beam_size, search_time, target_vocab, n = 10):\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seq)\n",
    "        \n",
    "        #here we track the strings still in our beam\n",
    "        track = [None]*beam_size\n",
    "        \n",
    "        # in worst case, take beam_size candidates from one former state\n",
    "        poss_next = np.min([beam_size, len(target_vocab)])\n",
    "        \n",
    "        # next states that we might want to keep around\n",
    "        possible_next = [None]*(beam_size*poss_next)\n",
    "        \n",
    "        # list of complete strings to keep around\n",
    "        final_candidates = []\n",
    "        \n",
    "        # first state we have in our beam\n",
    "        track[0] = ([target_vocab.stoi[\"<s>\"]], hidden, 0)\n",
    "        \n",
    "        #start time\n",
    "        start = time.time()\n",
    "        \n",
    "        #track no. of steps we've taken\n",
    "        steps = 0\n",
    "        \n",
    "        #continue looping until we've used search time\n",
    "        while time.time() < search_time:\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            #track where we are in the list of possible next values\n",
    "            poss_counter = 0\n",
    "            \n",
    "            for i in range(len(track)):\n",
    "                \n",
    "                att_probs = torch.matmul(encoder_output[:,0,:],track[i][1][0][self.last_hidden_enc,0,:])\n",
    "            \n",
    "                context = torch.matmul(att_probs,encoder_output[:,0,:])\n",
    "                \n",
    "                output, hidden = self.decoder(torch.Tensor(track[i][-1]), hidden, context)\n",
    "                \n",
    "                log_output = torch.log(output)\n",
    "                \n",
    "                top_next, idx = torch.topk(log_output, poss_next)\n",
    "                \n",
    "                for j in range(len(top_next)):\n",
    "                    \n",
    "                    if idx[j] != target_vocab.stoi[\"</s>\"]:\n",
    "                        \n",
    "                        possible_next[poss_counter+j] = (log_output[idx[j]] + track[i][2],top_next[j], idx[j], i, steps)\n",
    "                    \n",
    "                        poss_counter += 1\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        #we have a complete string\n",
    "                        final_candidates += [(top_next[j] + track[i][2],track[i][0]+[idx[j]])]\n",
    "                    \n",
    "                    \n",
    "            if poss_counter < beam_search:\n",
    "                \n",
    "                print(\"less possible next states than beam size. Seems unlikely this would ever happen.\")\n",
    "                \n",
    "                break\n",
    "                \n",
    "            #sort our possible next states and choose\n",
    "            hq.heapify(possible_next[:poss_counter])\n",
    "            largest = hq.nlargest(beam_size, possible_next[:poss_counter])\n",
    "            \n",
    "            #now loop over and put everything we want to keep in our track array\n",
    "            for k in range(poss_counter):\n",
    "                parent = track[possible_next[k][i]]\n",
    "                track[k] = (parent[0] + [possible_next[k][1]], hidden, possible_next[k][0])\n",
    "                \n",
    "        #finally, sort our final candidates\n",
    "        hq.heapify(final_candidates)\n",
    "        final = hq.nlargest(n, final_candidates)\n",
    "        \n",
    "        return final\n",
    "                \n",
    "            \n",
    "        \n",
    "    def predict(self, val_iter, vocab_size):\n",
    "        self.eval()\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        total_loss = 0\n",
    "        for batch in val_iter:\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source = Variable(source.data.cuda(), volatile=True)\n",
    "                target = Variable(target.data.cuda(), volatile=True)\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            total_loss += loss.data[0]\n",
    "        return total_loss / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, context_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=None):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.context_size = context_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size+self.context_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(self.hidden_size+self.context_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_seq, hidden, context):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        combined = torch.cat([embedded, context.unsqueeze(0)],2)\n",
    "        output, hidden = self.lstm(combined, hidden)\n",
    "        output = F.tanh(output)\n",
    "        output = self.softmax(self.out(torch.cat([output,context.unsqueeze(0)],2)))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 13352, hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "        \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=None):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = F.tanh(output)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None)\n",
    "decoder = AttnDecoderLSTM(hidden_size=200, context_size = 200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "seq2seq = Seq2SeqAttn(encoder, decoder).cuda()\n",
    "epoch_num = 13\n",
    "optimizer = optim.SGD(seq2seq.parameters(), lr=1)\n",
    "scheduler = MultiStepLR(optimizer, milestones=range(9, epoch_num), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a1eac4ec996c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
      "\u001b[0;32m<ipython-input-11-a26ef5c3a0b5>\u001b[0m in \u001b[0;36mbatch_train\u001b[0;34m(self, optimizer, train_iter, vocab_size, grad_clip)\u001b[0m\n\u001b[1;32m     66\u001b[0m                                    \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                                    ignore_index=pad)\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 10)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        if not os.path.isdir(\".save\"):\n",
    "            os.makedirs(\".save\")\n",
    "        torch.save(seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000][loss: 5.18][pp:177.07][time:661.47]\n",
      "[2000][loss: 4.93][pp:138.02][time:656.48]\n",
      "[3000][loss: 4.78][pp:119.11][time:652.24]\n",
      "[Epoch:0] val_loss:4.500 | val_pp:90.04S\n",
      "[!] saving model...\n",
      "[1000][loss: 4.51][pp:90.75][time:648.96]\n",
      "[2000][loss: 4.40][pp:81.60][time:653.10]\n",
      "[3000][loss: 4.32][pp:75.41][time:648.10]\n",
      "[Epoch:1] val_loss:4.173 | val_pp:64.90S\n",
      "[!] saving model...\n",
      "[1000][loss: 4.20][pp:66.53][time:649.66]\n",
      "[2000][loss: 4.10][pp:60.32][time:644.87]\n",
      "[3000][loss: 4.06][pp:58.19][time:649.94]\n",
      "[Epoch:2] val_loss:4.241 | val_pp:69.49S\n",
      "[1000][loss: 4.03][pp:56.35][time:646.50]\n",
      "[2000][loss: 3.98][pp:53.44][time:648.41]\n",
      "[3000][loss: 3.91][pp:49.93][time:649.93]\n",
      "[Epoch:3] val_loss:3.862 | val_pp:47.58S\n",
      "[!] saving model...\n",
      "[1000][loss: 3.79][pp:44.34][time:648.55]\n",
      "[2000][loss: 3.80][pp:44.86][time:647.78]\n",
      "[3000][loss: 3.74][pp:42.06][time:650.32]\n",
      "[Epoch:4] val_loss:3.662 | val_pp:38.96S\n",
      "[!] saving model...\n",
      "[1000][loss: 3.65][pp:38.66][time:649.32]\n",
      "[2000][loss: 3.68][pp:39.72][time:647.94]\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "seq2seq.load_state_dict(torch.load(\"./.save/seq2seq_0.pt\"))\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 10)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        if not os.path.isdir(\".save\"):\n",
    "            os.makedirs(\".save\")\n",
    "        torch.save(seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
