{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import numpy as np\n",
    "import torch.nn.init as weight_init\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7f39995032b0>, 'src': <torchtext.data.field.Field object at 0x7f39a7966a20>}\n",
      "119076\n",
      "{'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.'], 'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "    39   1126      8     12    241    277     12     12     40     28     73\n",
      "   115    267     86      5   2081    101      0     37     48      0    751\n",
      "   198      0   1124   1691     22    823   3038    126   2486     10     13\n",
      "  1333     24   4178      4      0    445   4828    115     15      3     15\n",
      "     3    381     29    226     36    116     33    368      0      8     30\n",
      "   163      0     62     36      5     10     19      3    542     11   6787\n",
      "    10   3286    275     46    946     52     19     10      5     67     78\n",
      "   118   1126    423     22      8     22   9826      4     49     22      5\n",
      "  7300    127   1567   3823   1626   3171      0      7   2591    324     21\n",
      "     4    507      2    259   7558    829    729      0     54      0    852\n",
      "     2      2    315      2      2      2      2     16      2      2      2\n",
      "\n",
      "Columns 11 to 21 \n",
      "    12    148     28     76    393     12     20     77    102     12     23\n",
      "    11      3     33    187   5131     83     88      4     59     21      4\n",
      "    24     11     49    269      3     72     53      3     74   1077    166\n",
      "   372    143     58     41    525   2005     56     75      3    194    514\n",
      "     7     82    113   2208   8047     88    654     21     31     41      3\n",
      "     0   3481   6197      8     21     44  11095    232    126    108      6\n",
      "    16      2     29   1982      4   1884      3     65    310     37      4\n",
      "     0    159      0     84    160     15      0     70    616     22      6\n",
      "   140      2   3784    165    338     71     62   4505   2076   1789   1239\n",
      "  4121    159   2448    158   2136    632   7430    911     54   8600    132\n",
      "    16      2      2      2      2      2      2     16      2      2      2\n",
      "\n",
      "Columns 22 to 31 \n",
      "    12     20     99     12   4404    102     12     76     76     77\n",
      "   134    201    896     49      9   8835      0   1972      0      4\n",
      "   222     18   8411    175      3     70     25    136     32      5\n",
      "   789      3      4      5     31      2   1608   1481      3      0\n",
      "  2253     43    339    883     44   3788    570     14      8    507\n",
      "   538      6   9381     86   6526     10    110    361   1681     16\n",
      " 10092   1530    637    384     49   8909     11   3209    236     26\n",
      "  1092    427      7   5247      5     81     65     48   3473      4\n",
      "   127      3    641   9300      0   3733   9045      0     41     19\n",
      "  4931   5147    339   6514   1546   8025   1290   1466   1023  10745\n",
      "     2      2      2  11930      2      2      2      2      2      2\n",
      "[torch.LongTensor of size 11x32]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 10 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "   367  10249     48     14     18    192     14     14     41     34     24\n",
      "    54    357    347      6     15    135    434     85     67  11345     19\n",
      "   102      0     88    597     43   4276     12    151      5     16    199\n",
      "   130     39      0     11     58     11   2745     55   3483    222      7\n",
      "    16     20     59    800      0     46     49     74      0      5      6\n",
      "    12  11196     49     29      7     35    336     16      5     18   2398\n",
      "     0     11     50     94      6   1426     28     12     67     10     55\n",
      "     7   1030    224      8    770   1362     58      6     25     28     13\n",
      "    40    807    503   1464     18      4   8538   3450    971      8     15\n",
      "    44     38      4    123      7      3    432     47      7    131    369\n",
      "     4    924     22      4     82      1      7     21     40   4846      4\n",
      "     3      4      3      3      0      1      8      3   3643   3240      3\n",
      "     1      3      1      1      4      1    722      1      4      5      1\n",
      "     1      1      1      1      3      1   1683      1      3    290      1\n",
      "     1      1      1      1      1      1      4      1      1      4      1\n",
      "     1      1      1      1      1      1      3      1      1      3      1\n",
      "\n",
      "Columns 11 to 21 \n",
      "     2      2      2      2      2      2      2      2      2      2      2\n",
      "    10     24     24    214   4319     14     10     89     52     14     27\n",
      "   101     10      5    453      0    234     23    108     36     16     12\n",
      "     5     90     19     35   3374     92   3083     31     72   1151     37\n",
      "    33      7     28     13    557     96     71     43     13      6      8\n",
      "     6    504    114    608   3524      5      9    329     44    164    914\n",
      "   550     15   3355     18      6     20      8     35     12     56      4\n",
      "   798      4     67     98    755   1944    442    122      8      8     48\n",
      "    21    112      5    149     15    441      5    204    142    750     11\n",
      "   595     15      9   5730     25      8   9972     21      9   9041    235\n",
      "    15      4   4199    272     63    748    935      3    260      4    147\n",
      "  3558    112   1930      4     37      4    348      1    705      3      4\n",
      "    21     15      4      3   1653      3      4      1    161      1      3\n",
      "     3      4      3      1      4      1      3      1      4      1      1\n",
      "     1      3      1      1      3      1      1      1      3      1      1\n",
      "     1      1      1      1      1      1      1      1      1      1      1\n",
      "     1      1      1      1      1      1      1      1      1      1      1\n",
      "\n",
      "Columns 22 to 31 \n",
      "     2      2      2      2      2      2      2      2      2      2\n",
      "    14     10     42     14   3501     52     14     14     24     89\n",
      "     6     26    473     67     13    268    497     81     81     12\n",
      "   178     30   1972     23      6    122   1825     23    307      6\n",
      "   234     62     11      6   3743      4    523   1183    158    153\n",
      "    13     32     17   5379     67     27      5     16      5   2665\n",
      "     5      8     45      9     28     36     16     17     18    455\n",
      "   202   3801    585     20   4795     40     79    104     81     21\n",
      "  5206    292   2207    168     33   1276    145   3050     12     27\n",
      "  1141      4  11105   2123     68     99   5483     17   7515     12\n",
      "   821      3      4   3925      4    589   4467      6    150      8\n",
      "    60      1      3    152      3      4      4   9215   2931   1337\n",
      "  3076      1      1     22      1      3      3   1238     71    591\n",
      "     4      1      1   4153      1      1      1      4     51      4\n",
      "     3      1      1      4      1      1      1      3     20      3\n",
      "     1      1      1     22      1      1      1      1      4      1\n",
      "     1      1      1      3      1      1      1      1      3      1\n",
      "[torch.LongTensor of size 17x32]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'source_test.txt' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seqs, target_seqs):\n",
    "        batch_size = input_seqs.size(1)\n",
    "        target_length = target_seqs.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        if use_cuda:\n",
    "            outputs = Variable(torch.zeros(target_length, batch_size, vocab_size)).cuda()\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(target_length, batch_size, vocab_size))\n",
    "        encoder_output, hidden = self.encoder(input_seqs, None)\n",
    "        output = Variable(target_seqs.data[0, :]) \n",
    "        for t in range(1, target_length):\n",
    "            output, hidden = self.decoder(output, hidden)\n",
    "            outputs[t] = output\n",
    "            #best = output[0].data.max(1)[1]\n",
    "            if use_cuda:\n",
    "                output = Variable(target_seqs.data[t]).cuda()\n",
    "                #output = Variable(best).cuda()\n",
    "            else:\n",
    "                output = Variable(target_seqs.data[t])\n",
    "                #output = Variable(best)\n",
    "        return outputs\n",
    "    \n",
    "    def batch_train(self, optimizer, train_iter, vocab_size, grad_clip=10):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        curr_time = time.time()\n",
    "        loss_f = nn.NLLLoss()\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(self.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if b % 1000 == 0 and b != 0:\n",
    "                total_loss = total_loss / 1000\n",
    "                print(\"[%d][loss:%5.2f][pp:%5.2f][time:%5.2f]\" %\n",
    "                      (b, total_loss, math.exp(total_loss), time.time() - curr_time))\n",
    "                total_loss = 0\n",
    "                curr_time = time.time()\n",
    "                \n",
    "                \n",
    "    def predict(self, val_iter, vocab_size):\n",
    "        self.eval()\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        total_loss = 0\n",
    "        for batch in val_iter:\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source = Variable(source.data.cuda(), volatile=True)\n",
    "                target = Variable(target.data.cuda(), volatile=True)\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            total_loss += loss.data[0]\n",
    "        return total_loss / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 13352, hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "        \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=None):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = F.tanh(output)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None)\n",
    "decoder = DecoderLSTM(hidden_size=200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "seq2seq = Seq2Seq(encoder, decoder).cuda()\n",
    "epoch_num = 13\n",
    "optimizer = optim.SGD(seq2seq.parameters(), lr=1)\n",
    "scheduler = MultiStepLR(optimizer, milestones=range(9, epoch_num), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 10)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        if not os.path.isdir(\".save\"):\n",
    "            os.makedirs(\".save\")\n",
    "        torch.save(seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
