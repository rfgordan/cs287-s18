{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 3: Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a full neural machine translation system using an attention-based encoder-decoder network to translate from German to English. The encoder-decoder network with attention forms the backbone of many current text generation systems. See [Neural Machine Translation and Sequence-to-sequence Models: A Tutorial](https://arxiv.org/pdf/1703.01619.pdf) for an excellent tutorial that also contains many modern advances.\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a non-attentional baseline model (pure seq2seq as in [ref](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)). \n",
    "2. Incorporate attention into the baseline model ([ref](https://arxiv.org/abs/1409.0473) but with dot-product attention as in class notes).\n",
    "3. Implement beam search: review/tutorial [here](http://www.phontron.com/slides/nlp-programming-en-13-search.pdf)\n",
    "4. Visualize the attention distribution for a few examples. \n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n",
    "\n",
    "This will be the most time-consuming assignment in terms of difficulty/training time, so we recommend that you get started early!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library and methods for pretrained word embeddings\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import torch\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "import numpy as np\n",
    "import torch.nn.init as weight_init\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to process the raw data using a tokenizer. We are going to be using spacy, which can be installed via:  \n",
    "  `[sudo] pip install spacy`  \n",
    "  \n",
    "Tokenizers for English/German can be installed via:  \n",
    "  `[sudo] python -m spacy download en`  \n",
    "  `[sudo] python -m spacy download de`\n",
    "  \n",
    "This isn't *strictly* necessary, and you can use your own tokenization rules if you prefer (e.g. a simple `split()` in addition to some rules to acccount for punctuation), but we recommend sticking to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we need to add the beginning-of-sentence token `<s>` and the end-of-sentence token `</s>` to the \n",
    "target so we know when to begin/end translating. We do not need to do this on the source side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) # only target needs BOS/EOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. This may take a few minutes.\n",
    "\n",
    "**While this dataset of 200K sentence pairs is relatively small compared to others, it will still take some time to train. So we are going to be only working with sentences of length at most 20 for this homework. Please train only on this reduced dataset for this homework.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trg': <torchtext.data.field.Field object at 0x7f12a43abfd0>, 'src': <torchtext.data.field.Field object at 0x7f12a7433f28>}\n",
      "119076\n",
      "{'trg': ['David', 'Gallo', ':', 'This', 'is', 'Bill', 'Lange', '.', 'I', \"'m\", 'Dave', 'Gallo', '.'], 'src': ['David', 'Gallo', ':', 'Das', 'ist', 'Bill', 'Lange', '.', 'Ich', 'bin', 'Dave', 'Gallo', '.']}\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 20\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)\n",
    "print(train.fields)\n",
    "print(len(train))\n",
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build the vocabulary and convert the text corpus into indices. We are going to be replacing tokens that occurred less than 5 times with `<unk>` tokens, and take the rest as our vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 113253), (',', 67237), ('ist', 24189), ('die', 23778), ('das', 17102), ('der', 15727), ('und', 15622), ('Sie', 15085), ('es', 13197), ('ich', 12946)]\n",
      "Size of German vocab 13353\n",
      "[('.', 113433), (',', 59512), ('the', 46029), ('to', 29177), ('a', 27548), ('of', 26794), ('I', 24887), ('is', 21775), (\"'s\", 20630), ('that', 19814)]\n",
      "Size of English vocab 11560\n",
      "2 3\n"
     ]
    }
   ],
   "source": [
    "MIN_FREQ = 5\n",
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "print(DE.vocab.freqs.most_common(10))\n",
    "print(\"Size of German vocab\", len(DE.vocab))\n",
    "print(EN.vocab.freqs.most_common(10))\n",
    "print(\"Size of English vocab\", len(EN.vocab))\n",
    "print(EN.vocab.stoi[\"<s>\"], EN.vocab.stoi[\"</s>\"]) #vocab index for <s>, </s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split our data into batches as usual. Batching for MT is slightly tricky because source/target will be of different lengths. Fortunately, `torchtext` lets you do this by allowing you to pass in a `sort_key` function. This will minimizing the amount of padding on the source side, but since there is still some padding you will inadvertendly \"attend\" to these padding tokens. \n",
    "\n",
    "One way to get rid of padding is to pass a binary `mask` vector to your attention module so its attention score (before the softmax) is minus infinity for the padding token. Another way (which is how we do it for our projects, e.g. opennmt) is to manually sort data into batches so that each batch has exactly the same source length (this means that some batches will be less than the desired batch size, though).\n",
    "\n",
    "However, for this homework padding won't matter too much, so it's fine to ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=BATCH_SIZE, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see that the BOS/EOS token is indeed appended to the target (English) sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      " 4087   159   159     0   159  1483   159   159   176   204   159   159  3038\n",
      "    2     2     2    16   122     2     2     2    16     2     2     2     2\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 13 to 25 \n",
      "  159   159   159     0   159  2227     0   159   159   266  9632   159  1213\n",
      "    2     2     2     2     2     2     2     2     2     2     2     2   122\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 26 to 38 \n",
      "  159   159   743   159   775  2227    77   159   159   159   159   159     0\n",
      "    2     2     2     2     2     2    16     2     2     2     2     2     2\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 39 to 51 \n",
      "  159   159   204    73   312  7262   159   159   159   159     0  5802   204\n",
      "  122     2     2  6455     2     2     2     2     2     2   122     2     2\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 52 to 63 \n",
      "  298   159   159   155   155   155   456   155   155    20  4226   155\n",
      "    2     2     2   142   142   142   461   142   142  9946   281   142\n",
      "    1     1     1     2     2     2     2     2     2     2     2     2\n",
      "[torch.LongTensor of size 3x64]\n",
      "\n",
      "Target\n",
      "Variable containing:\n",
      "\n",
      "Columns 0 to 12 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  175   112   112  4311   733  4016   112   112   209  1459   112   112  9198\n",
      "   45    15    15    13   187     4    15    15    21  1922    15    15     4\n",
      " 1744     4     4    21     3     3     4     4     3    39     4     4     3\n",
      "   45     3     3     3     1     1     3     3     1   397     3     3     1\n",
      " 3907     1     1     1     1     1     1     1     1     4     1     1     1\n",
      "    4     1     1     1     1     1     1     1     1     3     1     1     1\n",
      "    3     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 13 to 25 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  112   733   112     0   112   112     0   733   112   216     0   112  1143\n",
      "   15     4    15     9    15    15     4     4    15     4   472    15     4\n",
      "    4     3     4  4383     4     4     3     3     4     3     4     4     3\n",
      "    3     1     3     4     3     3     1     1     3     1     3     3     1\n",
      "    1     1     1     3     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 26 to 38 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  112   112   290   112     0   733    89   112   112   112   112   112     0\n",
      "   15    15     4    15     4     4    21    15    15    15    15    15     4\n",
      "    4     4     3     4     3     3     3     4     4     4     4     4     3\n",
      "    3     3     1     3     1     1     1     3     3     3     3     3     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 39 to 51 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  112   112   376    24     0  7054   112   112   112   112     0  4000   397\n",
      "   15    15     4    63     4    55    15    15    15    15   187    39     4\n",
      "    4     4     3     3     3     4     4     4     4     4     3  2169     3\n",
      "    3     3     1     1     1     3     3     3     3     3     1     4     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     3     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
      "\n",
      "Columns 52 to 63 \n",
      "    2     2     2     2     2     2     2     2     2     2     2     2\n",
      "  266   112   112   733   112   112   244   112   112    10  4344   112\n",
      "    4    15    15    53    15    15   105    15    15    75    56    15\n",
      "    3     4     4   107    53    53     4     4     4  3558   169    53\n",
      "    1     3     3     4   107   107     3     3     3     4     4   107\n",
      "    1     1     1     3     4     4     1     1     1     3     3     4\n",
      "    1     1     1     1     3     3     1     1     1     1     1     3\n",
      "    1     1     1     1     1     1     1     1     1     1     1     1\n",
      "[torch.LongTensor of size 8x64]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "print(\"Source\")\n",
    "print(batch.src)\n",
    "print(\"Target\")\n",
    "print(batch.trg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Now that we've processed the data, we are ready to begin modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "Now it is your turn to build the models described at the top of the assignment. \n",
    "\n",
    "When a model is trained, use the following test function to produce predictions, and then upload to the kaggle competition: https://www.kaggle.com/c/cs287-hw3-s18/\n",
    "\n",
    "For the final Kaggle test, we will provide the source sentence, and you are to predict the **first three words of the target sentence**. The source sentence can be found under `source_test.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open 'source_test.txt' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!head source_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to HW1, you are to predict the 100 most probable 3-gram that will begin the target sentence. The submission format will be as follows, where each word in the 3-gram will be separated by \"|\", and each 3-gram will be separated by space. For example, here is what an example submission might look like with 5 most-likely 3-grams (instead of 100)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "id,word\n",
    "1,Newspapers|talk|about When|I|was Researchers|call|the Twentysomethings|like|Alex But|before|long\n",
    "2,That|'s|what Newspapers|talk|about You|have|robbed It|'s|realizing My|parents|wanted\n",
    "3,We|forget|how We|think|about Proust|actually|links Does|any|other This|is|something\n",
    "4,But|what|do And|it|'s They|'re|on My|name|is It|only|happens\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you print out your data, you will need to escape quotes and commas with the following command so that Kaggle does not complain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
    "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, context_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=False):\n",
    "        super(AttnDecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.context_size = context_size\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size+self.context_size, self.hidden_size, self.n_layers, bidirectional=bidirectional)\n",
    "        for param in self.lstm.parameters():\n",
    "            nn.init.uniform(param, -0.08, 0.08)\n",
    "        self.out = nn.Linear(self.hidden_size+self.context_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_seq, hidden, context):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        #print(embedded.size(),context.size())\n",
    "        combined = torch.cat([embedded, context.permute(1,0,2)],2)\n",
    "        output, hidden = self.lstm(combined, hidden)\n",
    "        output = output.squeeze(0)\n",
    "        output = self.softmax(self.out(torch.cat([output,context.squeeze(1)],1)))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seqs, target_seqs):\n",
    "        batch_size = input_seqs.size(1)\n",
    "        target_length = target_seqs.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        if use_cuda:\n",
    "            outputs = Variable(torch.zeros(target_length, batch_size, vocab_size)).cuda()\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(target_length, batch_size, vocab_size))\n",
    "        encoder_output, hidden = self.encoder(input_seqs, None)\n",
    "        output = Variable(target_seqs.data[0, :]) \n",
    "        for t in range(1, target_length):\n",
    "            output, hidden = self.decoder(output, hidden)\n",
    "            outputs[t] = output\n",
    "            #best = output[0].data.max(1)[1]\n",
    "            if use_cuda:\n",
    "                output = Variable(target_seqs.data[t]).cuda()\n",
    "                #output = Variable(best).cuda()\n",
    "            else:\n",
    "                output = Variable(target_seqs.data[t])\n",
    "                #output = Variable(best)\n",
    "        return outputs\n",
    "    \n",
    "    def batch_train(self, optimizer, train_iter, vocab_size, grad_clip=2):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        curr_time = time.time()\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            source = flip(batch.src, 0)\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(self.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if b % 1000 == 0 and b != 0:\n",
    "                total_loss = total_loss / 1000\n",
    "                print(\"[%d][loss:%5.2f][pp:%5.2f][time:%5.2f]\" %\n",
    "                      (b, total_loss, math.exp(total_loss), time.time() - curr_time))\n",
    "                total_loss = 0\n",
    "                curr_time = time.time()\n",
    "                \n",
    "                \n",
    "    def predict(self, val_iter, vocab_size):\n",
    "        self.eval()\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        total_loss = 0\n",
    "        for batch in val_iter:\n",
    "            source = flip(batch.src, 0)\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source = Variable(source.data.cuda(), volatile=True)\n",
    "                target = Variable(target.data.cuda(), volatile=True)\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            total_loss += loss.data[0]\n",
    "        return total_loss / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size = 13352, hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = n_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx = 1)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, bidirectional=bidirectional, dropout = dropout)\n",
    "        for param in self.lstm.parameters():\n",
    "            nn.init.uniform(param, -0.08, 0.08)\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        return output, hidden\n",
    "        \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size=200, output_size=11560, n_layers = 2, dropout= 0.3, bidirectional=None):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx = 1)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.n_layers, bidirectional=bidirectional, dropout=dropout)\n",
    "        for param in self.lstm.parameters():\n",
    "            nn.init.uniform(param, -0.08, 0.08)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_seq, hidden):\n",
    "        embedded = self.embedding(input_seq).unsqueeze(0)\n",
    "        embedded = self.dropout(embedded)\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        output = output.squeeze(0)\n",
    "        output = self.softmax(self.out(output))\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=None)\n",
    "# decoder = DecoderLSTM(hidden_size=200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "# seq2seq = Seq2Seq(encoder, decoder).cuda()\n",
    "# epoch_num = 13\n",
    "# optimizer = optim.SGD(seq2seq.parameters(), lr=1)\n",
    "# scheduler = MultiStepLR(optimizer, milestones=range(9, epoch_num), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttn(nn.Module):\n",
    "    def __init__(self, encoder, decoder, use_true = True):\n",
    "        super(Seq2SeqAttn, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.last_hidden_enc = (self.encoder.num_layers*(self.encoder.bidirectional*2)) - 1\n",
    "        self.use_true = use_true # want to feed true last words in when training\n",
    "        \n",
    "    def forward(self, input_seqs, target_seqs, output_att = False):\n",
    "        batch_size = input_seqs.size(1)\n",
    "        target_length = target_seqs.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        self.use_true = not(output_att)\n",
    "        \n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seqs, None)\n",
    "        \n",
    "        output = Variable(target_seqs.data[0, :])\n",
    "        att_prob_list = []\n",
    "        sentence = [target_seqs.data[0, :][0]]\n",
    "        \n",
    "        if output_att == False:\n",
    "            outputs = Variable(torch.zeros(target_length, batch_size, vocab_size)).cuda()\n",
    "            for t in range(1, target_length):\n",
    "\n",
    "                #context = Variable(torch.zeros(batch_size,self.encoder.hidden_size)).cuda()\n",
    "\n",
    "                #print(context.size())\n",
    "\n",
    "                encoder_output_attn = encoder_output.transpose(0,1)\n",
    "                hidden_attn = hidden[0][self.last_hidden_enc,:,:]\n",
    "\n",
    "                hidden_attn=hidden_attn.unsqueeze(2)\n",
    "                #print(hidden_attn.size(), encoder_output_attn.size())\n",
    "                att_probs = torch.bmm(encoder_output_attn, hidden_attn)\n",
    "                att_prob_list.append(att_probs.cpu().data.numpy())\n",
    "                context = torch.bmm(att_probs.transpose(1,2), encoder_output_attn)\n",
    "    #             #loop over examples in batch\n",
    "    #             for i in range(batch_size):\n",
    "\n",
    "    #                 #print(encoder_output[:,i,:].size())\n",
    "    #                 #print(hidden[0][1,i,:].size())\n",
    "\n",
    "    #                 #calculate attention probabilities\n",
    "\n",
    "    #                 att_probs = torch.matmul(encoder_output[:,i,:],hidden[0][self.last_hidden_enc,i,:])\n",
    "\n",
    "    #                 #print(att_probs.size())\n",
    "\n",
    "    #                 #now get \"expected\" for each element in batch\n",
    "    #                 context[i,:] = torch.matmul(att_probs,encoder_output[:,i,:])\n",
    "\n",
    "\n",
    "                output, hidden = self.decoder(output, hidden, context)\n",
    "\n",
    "                outputs[t] = output\n",
    "\n",
    "                #use true values only if training/validating model\n",
    "                if self.use_true:\n",
    "                    output = Variable(target_seqs.data[t]).cuda()\n",
    "                else:\n",
    "                    output = Variable(output[0].data.max(0)[1]).cuda()\n",
    "                    sentence.append(output.data[0])\n",
    "                    if output.data[0] == 3:\n",
    "                        break\n",
    "        else:\n",
    "            t = 1\n",
    "            while True:\n",
    "\n",
    "                #context = Variable(torch.zeros(batch_size,self.encoder.hidden_size)).cuda()\n",
    "\n",
    "                #print(context.size())\n",
    "\n",
    "                encoder_output_attn = encoder_output.transpose(0,1)\n",
    "                hidden_attn = hidden[0][self.last_hidden_enc,:,:]\n",
    "\n",
    "                hidden_attn=hidden_attn.unsqueeze(2)\n",
    "                #print(hidden_attn.size(), encoder_output_attn.size())\n",
    "                att_probs = torch.bmm(encoder_output_attn, hidden_attn)\n",
    "                att_prob_list.append(att_probs.cpu().data.numpy())\n",
    "                context = torch.bmm(att_probs.transpose(1,2), encoder_output_attn)\n",
    "    #             #loop over examples in batch\n",
    "    #             for i in range(batch_size):\n",
    "\n",
    "    #                 #print(encoder_output[:,i,:].size())\n",
    "    #                 #print(hidden[0][1,i,:].size())\n",
    "\n",
    "    #                 #calculate attention probabilities\n",
    "\n",
    "    #                 att_probs = torch.matmul(encoder_output[:,i,:],hidden[0][self.last_hidden_enc,i,:])\n",
    "\n",
    "    #                 #print(att_probs.size())\n",
    "\n",
    "    #                 #now get \"expected\" for each element in batch\n",
    "    #                 context[i,:] = torch.matmul(att_probs,encoder_output[:,i,:])\n",
    "\n",
    "\n",
    "                output, hidden = self.decoder(output, hidden, context)\n",
    "\n",
    "                #use true values only if training/validating model\n",
    "                if self.use_true:\n",
    "                    output = Variable(target_seqs.data[t]).cuda()\n",
    "                else:\n",
    "                    output = Variable(output[0].data.max(0)[1]).cuda()\n",
    "                    sentence.append(output.data[0])\n",
    "                    if output.data[0] == 3:\n",
    "                        break\n",
    "                t += 1\n",
    "        if output_att:\n",
    "            return att_prob_list, sentence\n",
    "        return outputs\n",
    "    \n",
    "    def batch_train(self, optimizer, train_iter, vocab_size, grad_clip=10):\n",
    "        self.train()\n",
    "        total_loss = 0\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        curr_time = time.time()\n",
    "        for b, batch in enumerate(train_iter):\n",
    "            source = flip(batch.src, 0)\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source, target = source.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            loss.backward()\n",
    "            clip_grad_norm(self.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.data[0]\n",
    "\n",
    "            if b % 1000 == 0 and b != 0:\n",
    "                total_loss = total_loss / 1000\n",
    "                print(\"[%d][loss:%5.2f][pp:%5.2f][time:%5.2f]\" %\n",
    "                      (b, total_loss, math.exp(total_loss), time.time() - curr_time))\n",
    "                total_loss = 0\n",
    "                curr_time = time.time()\n",
    "    '''\n",
    "    def beam_search(self, input_seq, beam_size, search_time, target_vocab, n = 10):\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(input_seq, None)\n",
    "        \n",
    "        #here we track the strings still in our beam\n",
    "        track = [None]*beam_size\n",
    "        \n",
    "        # in worst case, take beam_size candidates from one former state\n",
    "        poss_next = np.min([beam_size, len(target_vocab)])\n",
    "        \n",
    "        # next states that we might want to keep around\n",
    "        possible_next = [None]*(beam_size*poss_next)\n",
    "        \n",
    "        # list of complete strings to keep around\n",
    "        final_candidates = []\n",
    "        \n",
    "        # first state we have in our beam\n",
    "        track[0] = ([target_vocab.stoi[\"<s>\"]], hidden, 0)\n",
    "        num_tracked = 1\n",
    "        \n",
    "        #start time\n",
    "        start = time.time()\n",
    "        \n",
    "        #track no. of steps we've taken\n",
    "        steps = 0\n",
    "        \n",
    "        #continue looping until we've used search time\n",
    "        while time.time() < start + search_time:\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            #track where we are in the list of possible next values\n",
    "            poss_counter = 0\n",
    "            \n",
    "            for i in range(num_tracked):\n",
    "                \n",
    "                att_probs = torch.matmul(encoder_output[:,0,:],track[i][1][0][self.last_hidden_enc,0,:].cuda())\n",
    "            \n",
    "                context = torch.matmul(att_probs,encoder_output[:,0,:].cuda())\n",
    "                \n",
    "                print(torch.LongTensor(track[i][0][-1]))\n",
    "                output, hidden = self.decoder(Variable(torch.LongTensor(track[i][0][-1])).cuda(), (x.cuda() for x in hidden), context.cuda())\n",
    "                \n",
    "                log_output = torch.log(output)\n",
    "                \n",
    "                top_next, idx = torch.topk(log_output, poss_next)\n",
    "                \n",
    "                for j in range(len(top_next)):\n",
    "                    \n",
    "                    if idx[j] != target_vocab.stoi[\"</s>\"]:\n",
    "                        \n",
    "                        possible_next[poss_counter+j] = (log_output[idx[j]] + track[i][2],top_next[j], idx[j], i, steps)\n",
    "                    \n",
    "                        poss_counter += 1\n",
    "                        \n",
    "                    else:\n",
    "                        \n",
    "                        #we have a complete string\n",
    "                        final_candidates += [(top_next[j] + track[i][2],track[i][0]+[idx[j]])]\n",
    "                    \n",
    "                    \n",
    "            if poss_counter < beam_search:\n",
    "                \n",
    "                print(\"less possible next states than beam size. Seems unlikely this would ever happen.\")\n",
    "                \n",
    "                break\n",
    "                \n",
    "            #sort our possible next states and choose\n",
    "            hq.heapify(possible_next[:poss_counter])\n",
    "            largest = hq.nlargest(beam_size, possible_next[:poss_counter])\n",
    "            \n",
    "            num_tracked = 0\n",
    "            \n",
    "            #now loop over and put everything we want to keep in our track array\n",
    "            for k in range(poss_counter):\n",
    "                parent = track[possible_next[k][i]]\n",
    "                track[k] = (parent[0] + [possible_next[k][1]], hidden, possible_next[k][0])\n",
    "                num_tracked += 1\n",
    "                \n",
    "        #finally, sort our final candidates\n",
    "        hq.heapify(final_candidates)\n",
    "        final = hq.nlargest(n, final_candidates)\n",
    "        \n",
    "        return final\n",
    "                \n",
    "    '''        \n",
    "        \n",
    "    def predict(self, val_iter, vocab_size):\n",
    "        self.eval()\n",
    "        pad = EN.vocab.stoi['<pad>']\n",
    "        total_loss = 0\n",
    "        for batch in val_iter:\n",
    "            source = batch.src\n",
    "            target = batch.trg\n",
    "            if use_cuda:\n",
    "                source = Variable(source.data.cuda(), volatile=True)\n",
    "                target = Variable(target.data.cuda(), volatile=True)\n",
    "            output = self.forward(source, target)\n",
    "            loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                                   target[1:].contiguous().view(-1),\n",
    "                                   ignore_index=pad)\n",
    "            total_loss += loss.data[0]\n",
    "        return total_loss / len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=False)\n",
    "decoder = AttnDecoderLSTM(hidden_size=200, context_size = 200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "seq2seq = Seq2SeqAttn(encoder, decoder).cuda()\n",
    "epoch_num = 5\n",
    "optimizer = optim.SGD(seq2seq.parameters(), lr=1)\n",
    "scheduler = MultiStepLR(optimizer, milestones=range(9, epoch_num), gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = None\n",
    "#seq2seq.load_state_dict(torch.load(\"./.save/seq2seq_4.pt\"))\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 4)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        torch.save(seq2seq.state_dict(), 'save/seq2seq_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "    scheduler.step()\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_iter:\n",
    "    True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "     9   2431     20  ...      26     20     23\n",
       "    47     21    168  ...       4    290      4\n",
       "   165     29    106  ...       4   1624      7\n",
       "        ...            â‹±           ...         \n",
       "  1874     22      0  ...       1      1      1\n",
       "     0      0    152  ...       1      1      1\n",
       "     2      2      2  ...       1      1      1\n",
       "[torch.LongTensor of size 20x58]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderLSTM(input_size = len(DE.vocab), hidden_size = 200, n_layers=2, dropout=0.3, bidirectional=False)\n",
    "decoder = AttnDecoderLSTM(hidden_size=200, context_size = 200, output_size = len(EN.vocab), n_layers = 2, dropout= 0.3, bidirectional=None)\n",
    "seq2seq = Seq2SeqAttn(encoder, decoder).cuda()\n",
    "seq2seq.load_state_dict(torch.load('seq2seq_a_11.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions, cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' '), rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words.split(' '))\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_weights = np.array(seq2seq.forward(batch.src[:, 4].unsqueeze(1).cuda(), batch.trg[:, 4].unsqueeze(1).cuda(), output_att = True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 0, 19, 198, 15, 32, 19, 138, 198, 15, 4, 3]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.forward(batch.src[:, 4].unsqueeze(1).cuda(), batch.trg[:, 4].unsqueeze(1).cuda(), output_att = True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 11, 1)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "att_weights[:-1, :,  1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1225688160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation = seq2seq.forward(batch.src[:, 55].unsqueeze(1).cuda(), batch.trg[:, 55].unsqueeze(1).cuda(), output_att = True)\n",
    "att_weights = np.array(translation[0])\n",
    "att_weights = np.array([item[0] for item in att_weights[:-1, :,  1:]])\n",
    "att_weights = att_weights.reshape((len(att_weights), att_weights.shape[1]))\n",
    "att_weights = np.array([softmax(item) for item in att_weights.T]).T\n",
    "showAttention(\" \".join([DE.vocab.itos[i] for i in batch.src[:, 55].data]), \" \".join([EN.vocab.itos[i] for i in translation[1][1:]]), att_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000][loss: 5.31][pp:201.41][time:205.36]\n",
      "[Epoch:0] val_loss:4.148 | val_pp:63.33\n",
      "[!] saving model...\n",
      "[1000][loss: 4.18][pp:65.34][time:202.74]\n",
      "[Epoch:1] val_loss:3.817 | val_pp:45.49\n",
      "[!] saving model...\n",
      "[1000][loss: 3.88][pp:48.46][time:205.51]\n",
      "[Epoch:2] val_loss:3.596 | val_pp:36.47\n",
      "[!] saving model...\n",
      "[1000][loss: 3.70][pp:40.64][time:202.14]\n",
      "[Epoch:3] val_loss:3.456 | val_pp:31.70\n",
      "[!] saving model...\n",
      "[1000][loss: 3.58][pp:35.75][time:203.62]\n",
      "[Epoch:4] val_loss:3.366 | val_pp:28.97\n",
      "[!] saving model...\n",
      "[1000][loss: 3.47][pp:32.26][time:203.44]\n",
      "[Epoch:5] val_loss:3.287 | val_pp:26.77\n",
      "[!] saving model...\n",
      "[1000][loss: 3.39][pp:29.80][time:205.18]\n",
      "[Epoch:6] val_loss:3.236 | val_pp:25.42\n",
      "[!] saving model...\n",
      "[1000][loss: 3.32][pp:27.57][time:203.59]\n",
      "[Epoch:7] val_loss:3.176 | val_pp:23.94\n",
      "[!] saving model...\n",
      "[1000][loss: 3.27][pp:26.19][time:206.35]\n",
      "[Epoch:8] val_loss:3.143 | val_pp:23.17\n",
      "[!] saving model...\n",
      "[1000][loss: 3.20][pp:24.60][time:203.93]\n",
      "[Epoch:9] val_loss:3.100 | val_pp:22.20\n",
      "[!] saving model...\n",
      "[1000][loss: 3.13][pp:22.96][time:205.12]\n",
      "[Epoch:10] val_loss:3.071 | val_pp:21.56\n",
      "[!] saving model...\n",
      "[1000][loss: 3.08][pp:21.86][time:205.74]\n",
      "[Epoch:11] val_loss:3.049 | val_pp:21.08\n",
      "[!] saving model...\n",
      "[1000][loss: 3.06][pp:21.36][time:203.70]\n",
      "[Epoch:12] val_loss:3.043 | val_pp:20.97\n",
      "[!] saving model...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_iter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-4c51e7759743>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq2seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[TEST] loss:%5.2f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_iter' is not defined"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "for i in range(epoch_num):\n",
    "    seq2seq.batch_train(optimizer, train_iter,len(EN.vocab), grad_clip = 2)\n",
    "    val_loss = seq2seq.predict(val_iter, len(EN.vocab))\n",
    "    print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2f\"\n",
    "          % (i, val_loss, math.exp(val_loss)))\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        print(\"[!] saving model...\")\n",
    "        torch.save(seq2seq.state_dict(), 'save/seq2seq_%d.pt' % (i))\n",
    "        best_val_loss = val_loss\n",
    "    scheduler.step()\n",
    "test_loss = seq2seq.predict(test_iter, len(EN.vocab))\n",
    "print(\"[TEST] loss:%5.2f\" % test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape(l):\n",
    "    return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should perform your hyperparameter search/early stopping/write-up based on perplexity, not the above metric. (In practice, people use a metric called [BLEU](https://www.aclweb.org/anthology/P02-1040.pdf), which is roughly a geometric average of 1-gram, 2-gram, 3-gram, 4-gram precision, with a brevity penalty for producing translations that are too short.)\n",
    "\n",
    "Finally, as always please put up a (short) write-up following the template provided in the repository:  https://github.com/harvard-ml-courses/cs287-s18/blob/master/template/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
