{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: RNNs in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals\n",
    "1. Build a simple RNN classifier\n",
    "2. Learn about PyTorch's in-built RNN modules (LSTM etc.)\n",
    "\n",
    "(Roughly follows http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Building an RNN sentiment classifier\n",
    "#### Part 1.1: Generating the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll generate some toy data. The task will be to recall an integer at a certain position in a sequence. \n",
    "For a sequence a<sub>1</sub> a<sub>12</sub> a<sub>3</sub> a<sub>4</sub> a<sub>5</sub> the output might be a<sub>3</sub>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of training examples\n",
    "n_train = 2000\n",
    "\n",
    "# number of validation examples\n",
    "n_val = 1000\n",
    "\n",
    "# length of each sequence\n",
    "n_length = 10\n",
    "\n",
    "# examples per batch\n",
    "n_batch = 32\n",
    "\n",
    "# size of the vocabulary\n",
    "n_vocab = 20\n",
    "\n",
    "# position to be recalled\n",
    "answer_pos = n_length-1\n",
    "\n",
    "# generate random sequences\n",
    "train_seq = Variable(torch.Tensor(n_train, n_length).random_(0, n_vocab).long())\n",
    "val_seq = Variable(torch.Tensor(n_val, n_length).random_(0, n_vocab).long())\n",
    "\n",
    "# choose the correct labels\n",
    "train_labels = train_seq.clone()[:, answer_pos]\n",
    "val_labels = val_seq.clone()[:, answer_pos]\n",
    "\n",
    "# group data into batches\n",
    "train_iter = []\n",
    "for i in range(0, n_train, n_batch):\n",
    "    batch_seq = train_seq[i:i+n_batch]\n",
    "    batch_labels = train_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        train_iter.append([batch_seq, batch_labels])\n",
    "    \n",
    "val_iter = []\n",
    "for i in range(0, n_val, n_batch):\n",
    "    batch_seq = val_seq[i:i+n_batch]\n",
    "    batch_labels = val_labels[i:i+n_batch]\n",
    "    if (batch_seq.size()[0] == n_batch):\n",
    "        val_iter.append([batch_seq, batch_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.2 Build the model (version 1)\n",
    "\n",
    "The RNN module will be a PyTorch model like any other, with init a forward functions. This network:\n",
    "1. Takes as input the word at a particular point in the sequence, as well as the hidden state at the previous state of the network\n",
    "2. Uses nn.Embedding to get a vector for the word\n",
    "3. Concatenate the embedding and the hidden state\n",
    "4. Apply a linear layer to get the next hidden state\n",
    "5. Apply a linear layer to get the output\n",
    "6. Output both "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, input_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        combined = torch.cat((embedded, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.3: Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can initialize and train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vector\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    for i in range(batch.size()[1]):\n",
    "        output, hidden = model(batch[:, i], hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output, label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the hidden vector\n",
    "n_hidden = 3\n",
    "\n",
    "# initialize the network\n",
    "rnn = RNN(n_vocab, n_hidden, n_vocab, n_vocab)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .05\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1.4: Test the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the model is similar to training it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_batch(batch, label):\n",
    "    if (batch.size()[0] != n_batch):\n",
    "        return 0, 0\n",
    "    \n",
    "    # initialize hidden state\n",
    "    hidden = Variable(torch.zeros(n_batch, n_hidden))\n",
    "    \n",
    "    # calculate forward pass\n",
    "    for i in range(batch[0].size()[0]):\n",
    "        output, hidden = rnn(batch[:, i], hidden)\n",
    "        \n",
    "    # calculate predictions\n",
    "    _, pred = output.max(1)\n",
    "\n",
    "    # calculate number of correct predictions\n",
    "    correct = (pred == label).long().sum().data[0]\n",
    "    return correct, n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then calculate the total score by looping through the batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Using PyTorch RNN modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch's RNN capabilities live [here](http://pytorch.org/docs/master/nn.html#recurrent-layers). We can use it as follows (note that the input is batched along the **second** dimension):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input = 10\n",
    "n_hidden = 20\n",
    "n_layers = 2\n",
    "n_batch = 3\n",
    "n_length = 5\n",
    "rnn = nn.LSTM(n_input, n_hidden, n_layers)\n",
    "input = Variable(torch.randn(n_length, n_batch, n_input))\n",
    "h0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "c0 = Variable(torch.randn(n_layers, n_batch, n_hidden))\n",
    "output, hn = rnn(input, (h0, c0))\n",
    "print(output, hn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a custom module to apply this module to our problem. This module will embed each integer, then apply the LSTM to the sequence, and then apply a linear and a softmax to get probabilities for each class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, vocab_size, n_layers):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        # embed the input integers\n",
    "        embedded = self.embedding(input)\n",
    "        \n",
    "        # put the batch along the second dimension\n",
    "        embedded = embedded.transpose(0, 1)\n",
    "        \n",
    "        # apply the LSTM\n",
    "        output, hidden = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # apply the linear and the softmax\n",
    "        output = self.softmax(self.linear(output))\n",
    "\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing are essentially the same as before, except that we no longer need to manually loop in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_batch(model, criterion, optim, batch, label):\n",
    "    # initialize hidden vectors\n",
    "    hidden = (Variable(torch.zeros(n_layers, n_batch, n_hidden)), Variable(torch.zeros(n_layers, n_batch, n_hidden)))\n",
    "\n",
    "    # clear gradients\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    # calculate forward pass\n",
    "    output, hidden = model(batch, hidden)\n",
    "\n",
    "    # calculate loss    \n",
    "    loss = criterion(output[answer_pos], label)\n",
    "\n",
    "    # backpropagate and step\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, criterion, optim):\n",
    "    for e in range(n_epochs):\n",
    "        batches = 0\n",
    "        epoch_loss = 0\n",
    "        avg_loss = 0\n",
    "        for batch, label in train_iter:\n",
    "            batch_loss = train_batch(model, criterion, optim, batch, label)\n",
    "            batches += 1\n",
    "            epoch_loss += batch_loss\n",
    "            avg_loss = ((avg_loss * (batches - 1)) + batch_loss) / batches\n",
    "        \n",
    "        print(\"Epoch \", e, \" Loss: \", epoch_loss)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of the embeddings and vectors\n",
    "n_embedding = 128\n",
    "n_hidden = 128\n",
    "\n",
    "# number of layers\n",
    "n_layers = 1\n",
    "\n",
    "# initialize LSTM\n",
    "rnn = MyLSTM(n_embedding, n_hidden, n_vocab, n_vocab, n_layers)\n",
    "\n",
    "n_epochs = 30\n",
    "learning_rate = .1\n",
    "criterion = nn.NLLLoss()\n",
    "optim = torch.optim.SGD(rnn.parameters(), lr = learning_rate)\n",
    "\n",
    "train(rnn, criterion, optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop\n",
    "\n",
    "batch_num = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(val_iter)):\n",
    "    batch, label = val_iter[i]\n",
    "    batch_correct, batch_size = test_batch(batch, label)\n",
    "    batch_num += 1\n",
    "    correct += batch_correct\n",
    "    total += batch_size\n",
    "    \n",
    "print(\"Percent correct: \", correct / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
