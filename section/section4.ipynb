{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this section, we will be using policy gradients (i.e. REINFORCE algorithm) to learn a hard-attention model on a synthetic task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.distributions #this package provides a lot of nice abstractions for policy gradients\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our synthetic task will consist of learning to copy a sequence of integers. Here is the function to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data(vocab_size = 50, len = 4, batch_size = 20):\n",
    "    x = np.random.randint(1, vocab_size, size = (batch_size, len)) #input data\n",
    "    y = x[:, ::]
    "    #target needs special \"start sentence\" token, which will have token idx = 0 (we don't need <eos> here though)\n",
    "    y = np.hstack([np.zeros((batch_size, 1)), y])\n",
    "    return Variable(torch.from_numpy(x).long()), Variable(torch.from_numpy(y).long())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_data()\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define our network, which will be an encoder/decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnNetwork(nn.Module):\n",
    "    def __init__(self, vocab_size = 50, word_dim = 50, hidden_dim = 300):\n",
    "        super(AttnNetwork, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = nn.LSTM(word_dim, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        self.decoder = nn.LSTM(word_dim, hidden_dim, num_layers = 1, batch_first = True)\n",
    "        self.embedding = nn.Embedding(vocab_size, word_dim) #we are going to be sharing the embedding layer \n",
    "        #this vocab layer will combine dec hidden state with context vector, and the project out into vocab space \n",
    "        self.vocab_layer = nn.Sequential(nn.Linear(hidden_dim*2, hidden_dim),\n",
    "                                         nn.Tanh(), nn.Linear(hidden_dim, vocab_size), nn.LogSoftmax())\n",
    "        #baseline reward, which we initialize with log 1/V\n",
    "        self.baseline = Variable(torch.zeros(1).fill_(np.log(1/vocab_size)))                \n",
    "        \n",
    "    def forward(self, x, attn_type=\"hard\"):\n",
    "        emb = self.embedding(x)\n",
    "        h0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim))\n",
    "        c0 = Variable(torch.zeros(1, x.size(0), self.hidden_dim))\n",
    "        enc_h, _ = self.encoder(emb, (h0, c0))\n",
    "        dec_h, _ = self.decoder(emb[:, :-1], (h0, c0))\n",
    "        #we've gotten our encoder/decoder hidden states so we are ready to do attention        \n",
    "        #first let's get all our scores, which we can do easily since we are using dot-prod attention\n",
    "        scores = torch.bmm(enc_h, dec_h.transpose(1,2)) #this will be a batch x source_len x target_len\n",
    "        neg_reward = 0\n",
    "        loss = 0\n",
    "        avg_reward = 0        \n",
    "        for t in range(dec_h.size(1)):            \n",
    "            attn_dist = F.softmax(scores[:, :, t], dim=1) #get attention scores\n",
    "            if attn_type == \"hard\":\n",
    "                cat = torch.distributions.Categorical(attn_dist) \n",
    "                attn_samples = cat.sample() #samples from attn_dist    \n",
    "                #make this into a one-hot distribution (there are more efficient ways of doing this)\n",
    "                one_hot = Variable(torch.zeros_like(attn_dist.data).scatter_(-1, attn_samples.data.unsqueeze(1), 1))\n",
    "                context = torch.bmm(one_hot.unsqueeze(1), enc_h).squeeze(1)                 \n",
    "            else:\n",
    "                context = torch.bmm(attn_dist.unsqueeze(1), enc_h).squeeze(1)\n",
    "            pred = self.vocab_layer(torch.cat([dec_h[:, t], context], 1))\n",
    "            y = x[:, t+1] #this will be our label\n",
    "            reward = torch.gather(pred, 1, y.unsqueeze(1))  #our reward is log prob at the word level\n",
    "            avg_reward += reward.data.mean() \n",
    "            if attn_type == \"hard\":                \n",
    "                neg_reward -= (cat.log_prob(attn_samples) * (reward.detach()-self.baseline)).mean() #reinforce rule                                        \n",
    "            loss -= reward.mean()       \n",
    "        avg_reward = avg_reward/dec_h.size(1)\n",
    "        self.baseline.data = 0.95*self.baseline.data + 0.05*avg_reward #update baseline as a moving average\n",
    "        return loss, neg_reward\n",
    "    \n",
    "    def predict(self, x, attn_type = \"hard\"):\n",
    "        #predict with greedy decoding\n",
    "        emb = self.embedding(x)\n",
    "        h = Variable(torch.zeros(1, x.size(0), self.hidden_dim))\n",
    "        c = Variable(torch.zeros(1, x.size(0), self.hidden_dim))\n",
    "        enc_h, _ = self.encoder(emb, (h, c))\n",
    "        y = [Variable(torch.zeros(x.size(0)).long())]\n",
    "        self.attn = []        \n",
    "        for t in range(x.size(1)):\n",
    "            emb_t = self.embedding(y[-1])\n",
    "            dec_h, (h, c) = self.decoder(emb_t.unsqueeze(1), (h, c))\n",
    "            scores = torch.bmm(enc_h, dec_h.transpose(1,2)).squeeze(2)\n",
    "            attn_dist = F.softmax(scores, dim = 1)\n",
    "            self.attn.append(attn_dist.data)\n",
    "            if attn_type == \"hard\":\n",
    "                _, argmax = attn_dist.max(1)\n",
    "                one_hot = Variable(torch.zeros_like(attn_dist.data).scatter_(-1, argmax.data.unsqueeze(1), 1))\n",
    "                context = torch.bmm(one_hot.unsqueeze(1), enc_h).squeeze(1)                    \n",
    "            else:                \n",
    "                context = torch.bmm(attn_dist.unsqueeze(1), enc_h).squeeze(1)\n",
    "            pred = self.vocab_layer(torch.cat([dec_h.squeeze(1), context], 1))\n",
    "            _, next_token = pred.max(1)\n",
    "            y.append(next_token)\n",
    "        self.attn = torch.stack(self.attn, 0).transpose(0, 1)\n",
    "        return torch.stack(y, 0).transpose(0, 1)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(t):\n",
    "    #print 2d tensors nicely\n",
    "    print(\"\\n\".join([\" \".join(list(map(\"{:.3f}\".format, list(u)))) for u in list(t)]))\n",
    "    print('')\n",
    "    \n",
    "model = AttnNetwork()\n",
    "attn_type = \"hard\"\n",
    "num_iters = 10000\n",
    "optim = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "avg_acc = 0\n",
    "for i in range(num_iters):\n",
    "    optim.zero_grad()\n",
    "    x, y = generate_data()\n",
    "    loss, neg_reward = model.forward(x, attn_type)\n",
    "    y_pred = model.predict(x, attn_type)\n",
    "    correct = torch.sum(y_pred.data[:, 1:] == y.data[:, 1:]) #exclude <s> token in acc calculation    \n",
    "    (loss + neg_reward).backward()\n",
    "    torch.nn.utils.clip_grad_norm(model.parameters(), 1)\n",
    "    optim.step()     \n",
    "    avg_acc = 0.95*avg_acc + 0.05*correct/ (x.size(0)*x.size(1))    \n",
    "    if i % 100 == 0:        \n",
    "        print(\"Attn Type: %s, Iter: %d, Reward: %.2f, Accuracy: %.2f, PPL: %.2f\" %\n",
    "              (attn_type, i, model.baseline.data[0], avg_acc, np.exp(loss/x.size(1))))\n",
    "        pretty_print(model.attn[0])        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
